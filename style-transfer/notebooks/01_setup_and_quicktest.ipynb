{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Neural Style Transfer - Setup & Quick Test\n",
    "\n",
    "This notebook will:\n",
    "1. Test a pre-trained PyTorch style transfer model\n",
    "2. Benchmark inference speed\n",
    "3. Compare quality with current cv::stylization output\n",
    "\n",
    "**Goal**: Validate that neural style transfer can replace cv::stylization with 30-120x speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Pre-trained Fast Style Transfer Model\n",
    "\n",
    "We'll use a pre-trained model from PyTorch Hub or download one directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined!\n"
     ]
    }
   ],
   "source": [
    "# Define the Fast Style Transfer model architecture\n",
    "# Based on Johnson et al. \"Perceptual Losses for Real-Time Style Transfer\"\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(padding)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "class UpsampleConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample_layer = nn.Upsample(scale_factor=upsample)\n",
    "        self.reflection_pad = nn.ReflectionPad2d(kernel_size // 2)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = self.upsample_layer(x)\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual blocks\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        # Decoder\n",
    "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.in1(self.conv1(x)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.in4(self.deconv1(y)))\n",
    "        y = self.relu(self.in5(self.deconv2(y)))\n",
    "        y = self.deconv3(y)\n",
    "        return y\n",
    "\n",
    "print(\"Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Pre-trained Model Weights\n",
    "\n",
    "We'll download a pre-trained model (mosaic style as a test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at ../models/mosaic.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hasRecord(\"version\") to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerNet()\n\u001b[0;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/livestream-morphing-fcVJWF4q-py3.10/lib/python3.10/site-packages/torch/serialization.py:797\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    799\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    800\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/livestream-morphing-fcVJWF4q-py3.10/lib/python3.10/site-packages/torch/serialization.py:283\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hasRecord(\"version\") to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download a pre-trained model (mosaic style as example)\n",
    "model_url = \"https://www.dropbox.com/s/lrvwfehqdcxoza8/mosaic.pth?dl=1\"\n",
    "model_path = models_dir / \"mosaic.pth\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f\"Downloading pre-trained model to {model_path}...\")\n",
    "    urllib.request.urlretrieve(model_url, model_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Model already exists at {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "model = TransformerNet()\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test on Abbey Road Frame\n",
    "\n",
    "Let's grab a test frame from your processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess image\n",
    "def load_image(image_path, size=None):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    if size:\n",
    "        img = img.resize((size, int(size * img.size[1] / img.size[0])), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def transform_image(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "def denormalize_image(tensor):\n",
    "    # Clamp to [0, 1] range\n",
    "    tensor = tensor.squeeze(0).clamp(0, 1)\n",
    "    return transforms.ToPILImage()(tensor.cpu())\n",
    "\n",
    "# Try to find a test frame from your data\n",
    "test_image_dir = Path(\"../../data/frames\")\n",
    "test_frames = list(test_image_dir.glob(\"**/*.jpg\"))\n",
    "\n",
    "if test_frames:\n",
    "    test_image_path = test_frames[0]\n",
    "    print(f\"Using test frame: {test_image_path}\")\n",
    "else:\n",
    "    print(\"No test frames found. Please add a frame to ../test_images/\")\n",
    "    test_image_path = None\n",
    "\n",
    "# If we have a test image, process it\n",
    "if test_image_path:\n",
    "    # Load original image\n",
    "    original_img = load_image(test_image_path)\n",
    "    print(f\"Original image size: {original_img.size}\")\n",
    "    \n",
    "    # Prepare input tensor\n",
    "    input_tensor = transform_image(original_img).to(device)\n",
    "    \n",
    "    # Benchmark inference time\n",
    "    warmup_runs = 5\n",
    "    test_runs = 20\n",
    "    \n",
    "    print(\"\\nWarming up GPU...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_runs):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    print(f\"Running {test_runs} inference tests...\")\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(test_runs):\n",
    "            start = time.time()\n",
    "            output_tensor = model(input_tensor)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            times.append((end - start) * 1000)  # Convert to ms\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"INFERENCE BENCHMARK RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Average inference time: {avg_time:.2f}ms Â± {std_time:.2f}ms\")\n",
    "    print(f\"Estimated time per 30-frame segment: {(avg_time * 30) / 1000:.2f}s\")\n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  Current cv::stylization: ~180s per segment\")\n",
    "    print(f\"  Neural style transfer:   ~{(avg_time * 30) / 1000:.2f}s per segment\")\n",
    "    print(f\"  Speedup:                 ~{180 / ((avg_time * 30) / 1000):.1f}x faster!\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Convert output to image\n",
    "    output_img = denormalize_image(output_tensor)\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title('Original Frame', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(output_img)\n",
    "    axes[1].set_title(f'Neural Style Transfer ({avg_time:.0f}ms)', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save output\n",
    "    output_dir = Path(\"../outputs\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    output_path = output_dir / \"quicktest_output.jpg\"\n",
    "    output_img.save(output_path)\n",
    "    print(f\"Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "If the speed looks good (should be 50-200ms per frame), we'll:\n",
    "\n",
    "1. **Train on Dali style** - Create a model trained on actual Dali paintings\n",
    "2. **Export to ONNX** - For C++ integration\n",
    "3. **Optimize with TensorRT** - For maximum GPU performance\n",
    "4. **Integrate into fast_processor.cpp** - Replace cv::stylization\n",
    "\n",
    "See notebook `02_train_dali_style.ipynb` for training instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export to ONNX (Optional - for C++ integration)\n",
    "\n",
    "Once we're happy with the model, we can export it to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Export model to ONNX\n",
    "if test_image_path:\n",
    "    onnx_path = models_dir / \"style_transfer.onnx\"\n",
    "    \n",
    "    # Create dummy input (1920x1080 like Abbey Road stream)\n",
    "    dummy_input = torch.randn(1, 3, 1080, 1920).to(device)\n",
    "    \n",
    "    print(f\"Exporting model to {onnx_path}...\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size', 2: 'height', 3: 'width'},\n",
    "                     'output': {0: 'batch_size', 2: 'height', 3: 'width'}}\n",
    "    )\n",
    "    \n",
    "    print(f\"ONNX model saved to {onnx_path}\")\n",
    "    print(f\"\\nNext: Optimize with TensorRT for maximum speed!\")\n",
    "    print(f\"Command: trtexec --onnx={onnx_path} --saveEngine=style_transfer.trt --fp16\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
