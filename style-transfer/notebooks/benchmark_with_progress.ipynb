{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer - Speed Benchmark with Progress\n",
    "\n",
    "This notebook will test the inference speed of neural style transfer on your hardware.\n",
    "\n",
    "**Goal**: Determine if neural style transfer can achieve <6s per 30-frame segment\n",
    "\n",
    "**Current cv::stylization**: ~180s per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from train import TransformerNet\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Device and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "âš ï¸  Running on CPU - this will be slower than GPU\n",
      "   For <6s target, GPU acceleration is recommended\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(\"\\nâš ï¸  Running on CPU - this will be slower than GPU\")\n",
    "    print(\"   For <6s target, GPU acceleration is recommended\")\n",
    "else:\n",
    "    print(f\"\\nâœ… GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"   Should achieve <6s target easily!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Fast Style Transfer model...\n",
      "âœ… Model created!\n",
      "   Parameters: 1,679,235\n",
      "   Model size: ~6.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(\"Creating Fast Style Transfer model...\")\n",
    "model = TransformerNet()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ… Model created!\")\n",
    "print(f\"   Parameters: {total_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Test Input (Abbey Road Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input resolution: 1920x1080 (Abbey Road stream)\n",
      "Total pixels: 2,073,600\n",
      "\n",
      "Input tensor shape: torch.Size([1, 3, 1080, 1920])\n",
      "Memory: ~23.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input matching Abbey Road stream resolution\n",
    "height, width = 1080, 1920\n",
    "print(f\"Input resolution: {width}x{height} (Abbey Road stream)\")\n",
    "print(f\"Total pixels: {height * width:,}\")\n",
    "\n",
    "dummy_input = torch.randn(1, 3, height, width).to(device)\n",
    "print(f\"\\nInput tensor shape: {dummy_input.shape}\")\n",
    "print(f\"Memory: ~{dummy_input.numel() * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Warmup (5 runs)\n",
    "\n",
    "First few runs are slower as the system initializes. We'll do warmup runs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 warmup iterations...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73f30dab59b492a9d2760e18b167297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_warmup = 5\n",
    "print(f\"Running {num_warmup} warmup iterations...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num_warmup), desc=\"Warmup\"):\n",
    "        output = model(dummy_input)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "print(\"\\nâœ… Warmup complete!\")\n",
    "print(f\"   Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark (20 runs with progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 20\n",
    "print(f\"Running {num_runs} benchmark iterations...\\n\")\n",
    "\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num_runs), desc=\"Benchmark\"):\n",
    "        start = time.perf_counter()\n",
    "        output = model(dummy_input)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        elapsed_ms = (end - start) * 1000\n",
    "        times.append(elapsed_ms)\n",
    "        \n",
    "        # Show running average\n",
    "        if (i + 1) % 5 == 0:\n",
    "            avg_so_far = np.mean(times)\n",
    "            tqdm.write(f\"   Avg after {i+1} runs: {avg_so_far:.2f}ms\")\n",
    "\n",
    "print(\"\\nâœ… Benchmark complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate statistics\n",
    "times = np.array(times)\n",
    "mean_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "min_time = np.min(times)\n",
    "max_time = np.max(times)\n",
    "median_time = np.median(times)\n",
    "\n",
    "# Plot distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series\n",
    "ax1.plot(times, 'o-', linewidth=2, markersize=6)\n",
    "ax1.axhline(mean_time, color='r', linestyle='--', label=f'Mean: {mean_time:.2f}ms')\n",
    "ax1.fill_between(range(len(times)), \n",
    "                  mean_time - std_time, \n",
    "                  mean_time + std_time, \n",
    "                  alpha=0.2, color='r')\n",
    "ax1.set_xlabel('Run Number', fontsize=12)\n",
    "ax1.set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "ax1.set_title('Inference Time per Run', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram\n",
    "ax2.hist(times, bins=15, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(mean_time, color='r', linestyle='--', linewidth=2, label=f'Mean: {mean_time:.2f}ms')\n",
    "ax2.axvline(median_time, color='g', linestyle='--', linewidth=2, label=f'Median: {median_time:.2f}ms')\n",
    "ax2.set_xlabel('Inference Time (ms)', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Distribution of Inference Times', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate segment timing\n",
    "frames_per_segment = 30\n",
    "segment_time = (mean_time * frames_per_segment) / 1000  # seconds\n",
    "\n",
    "# Current cv::stylization timing\n",
    "current_stylization_per_frame = 5500  # ms\n",
    "current_segment_time = (current_stylization_per_frame * frames_per_segment) / 1000  # seconds\n",
    "\n",
    "speedup = current_segment_time / segment_time\n",
    "\n",
    "# Print report\n",
    "print(\"=\" * 70)\n",
    "print(\"NEURAL STYLE TRANSFER BENCHMARK RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Resolution: {width}x{height} ({width * height:,} pixels)\")\n",
    "print()\n",
    "print(\"Inference Time Statistics (per frame):\")\n",
    "print(f\"  Mean:      {mean_time:>8.2f}ms Â± {std_time:.2f}ms\")\n",
    "print(f\"  Median:    {median_time:>8.2f}ms\")\n",
    "print(f\"  Min:       {min_time:>8.2f}ms\")\n",
    "print(f\"  Max:       {max_time:>8.2f}ms\")\n",
    "print()\n",
    "print(\"Time per 30-frame segment:\")\n",
    "print(f\"  Neural Style Transfer:  {segment_time:>6.2f}s\")\n",
    "print(f\"  cv::stylization:        {current_segment_time:>6.0f}s\")\n",
    "print()\n",
    "print(f\"SPEEDUP: {speedup:.1f}x faster! ðŸš€\")\n",
    "print()\n",
    "\n",
    "# Verdict\n",
    "if segment_time < 6.0:\n",
    "    print(\"âœ… SUCCESS! Meets <6s per segment target!\")\n",
    "    print(f\"   ({segment_time:.2f}s is well under the 6s goal)\")\n",
    "    print()\n",
    "    print(\"Next steps:\")\n",
    "    print(\"  1. Train model on Dali painting style\")\n",
    "    print(\"  2. Export to ONNX format\")\n",
    "    print(\"  3. Integrate into C++ pipeline\")\n",
    "elif segment_time < 30.0:\n",
    "    print(\"âš ï¸  Close but doesn't meet <6s target\")\n",
    "    print(f\"   ({segment_time:.2f}s is over the 6s goal)\")\n",
    "    print()\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"  1. Use GPU with CUDA for 10-30x speedup\")\n",
    "    print(\"  2. Or optimize with TensorRT INT8 quantization\")\n",
    "    print(\"  3. Or try lighter MobileNet-based architecture\")\n",
    "    print(\"  4. Or use hybrid keyframe approach (process every 3rd frame)\")\n",
    "else:\n",
    "    print(\"âŒ Does not provide significant speedup\")\n",
    "    print(f\"   ({segment_time:.2f}s vs 180s target)\")\n",
    "    print()\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"  1. GPU acceleration is essential\")\n",
    "    print(\"  2. Or explore alternative approaches:\")\n",
    "    print(\"     - Temporal caching (process keyframes only)\")\n",
    "    print(\"     - Pre-computed style LUT approximations\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Breakdown\n",
    "\n",
    "Let's see where the time is going by testing different resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing performance at different resolutions...\\n\")\n",
    "\n",
    "test_sizes = [\n",
    "    (540, 960, \"Half resolution\"),\n",
    "    (720, 1280, \"720p\"),\n",
    "    (1080, 1920, \"Full (1080p)\"),\n",
    "]\n",
    "\n",
    "resolution_results = []\n",
    "\n",
    "for h, w, name in test_sizes:\n",
    "    test_input = torch.randn(1, 3, h, w).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = model(test_input)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    test_times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            start = time.perf_counter()\n",
    "            _ = model(test_input)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            test_times.append((end - start) * 1000)\n",
    "    \n",
    "    avg_time = np.mean(test_times)\n",
    "    pixel_count = h * w\n",
    "    segment_time = (avg_time * 30) / 1000\n",
    "    \n",
    "    resolution_results.append({\n",
    "        'name': name,\n",
    "        'resolution': f\"{w}x{h}\",\n",
    "        'pixels': pixel_count,\n",
    "        'time_ms': avg_time,\n",
    "        'segment_s': segment_time\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:15s} ({w}x{h}): {avg_time:>7.2f}ms/frame â†’ {segment_time:>5.2f}s/segment\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ’¡ Insight: If lower resolution is acceptable, could process faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "methods = ['cv::stylization\\n(current)', f'Neural ST\\n({device.type})']\n",
    "times_comparison = [current_segment_time, segment_time]\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "\n",
    "bars = ax.bar(methods, times_comparison, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars, times_comparison):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{time_val:.1f}s',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add target line\n",
    "ax.axhline(6.0, color='green', linestyle='--', linewidth=2, label='Target: <6s')\n",
    "\n",
    "ax.set_ylabel('Time per Segment (seconds)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Processing Time Comparison\\n(30 frames per segment)', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add speedup annotation\n",
    "ax.text(0.5, max(times_comparison) * 0.9, \n",
    "        f'{speedup:.1f}x\\nSpeedup',\n",
    "        ha='center', fontsize=16, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now have concrete benchmark numbers for neural style transfer on your hardware.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- The model architecture is lightweight (~1.7M parameters)\n",
    "- Speed depends heavily on hardware (GPU vs CPU)\n",
    "- Even on CPU, likely 3-12x faster than cv::stylization\n",
    "- With GPU, easily achieves 30-120x speedup\n",
    "\n",
    "**Next:** Based on these results, decide whether to:\n",
    "1. Proceed with neural ST (if speed is acceptable)\n",
    "2. Add GPU acceleration (if available)\n",
    "3. Explore optimization techniques (TensorRT, INT8, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
